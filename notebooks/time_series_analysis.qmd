---
title: "Forecasting: Principles and practice the Pythonic way"
format: 
    pdf:
        number-sections: true 
        toc: true
execute: 
  echo: true
  output: false
engine: knitr
---


```{r data-sources}
# eval: false 
"""
CPI adjustment for inflation 
# https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1810000501&pickMembers%5B0%5D=1.2&cubeTimeFrame.startYear=1990&cubeTimeFrame.endYear=2024&referencePeriods=19900101%2C20240101
"""
```

```{r initiate-reticulate}
library('easypackages')
packages = c('reticulate', 'fpp3', 'tidyverse', 'fable', 'slider', 'seasonal', 'readr')
libraries(packages)
# install.packages('feasts')
use_condaenv("base", required = TRUE)

setwd('/Users/seb/Desktop/Projects/gas_pump_prices/')
```

```{python loading-packages}
import os
import tarfile
import urllib
import pandas as pd
import numpy as np
from plotnine import * 
from utilsforecast.plotting import plot_series_utils
from fpppy.utils import plot_series

from scipy.stats import *
import statsmodels.api as sm

os.chdir('/Users/seb/Desktop/Projects/gas_pump_prices/')
```

```{python load-data-pyt}
df_gas = pd.read_csv('data/raw/fueltypesall.csv')

# convert date to datetime object 
df_gas['Date'] = pd.to_datetime(df_gas['Date'])
```


```{r load-data-r}
df_gasr = read_csv('data/raw/fueltypesall.csv')

# convert date to tsibble and then to long format
df_gasr <- df_gasr |>
  pivot_longer(cols = -c(Date, `Fuel Type`, `Type de carburant`), 
               names_to = "city", values_to = "price") |> 
  mutate(week= yearweek(Date)) |>
  as_tsibble(index = week, key = c('city', `Fuel Type`))
```

Apply inflation correction to gas prices. 

```{r inflation-corr}
# read in inflation data
df_inflation = read_csv('data/raw/1810000401_databaseLoadingData.csv')
df_gas_inf = df_inflation |> filter(`Products and product groups` == 'Gasoline')

df_gasr$year_month = format(df_gasr$Date, "%Y-%m")

# compute dollars in terms of 2025 dollars
# 1) Compute CPIs centered on 2025 
value_25 = df_gas_inf |> filter(REF_DATE == '2025-06') |> pull(VALUE)
df_gas_inf$cpi_2025 = value_25/df_gas_inf$VALUE

# merge in CPI value based on year and compute 
cols = c('REF_DATE', 'VALUE', 'cpi_2025')
df_gasr <- left_join(df_gasr, df_gas_inf[ ,cols], by = c("year_month" = "REF_DATE"))

# examine if tiem period with missing cpi_2025 value exists in inflatino prices
missing_time_periods = unique(df_test[is.na(df_test$cpi_2025), 'year_month'])

# NOTE: replace 1990-01 with 1990-02 and 2025-07 with 2025-06
df_gasr[df_gasr$year_month == '1990-01', 'cpi_2025'] = unique(df_gasr[df_gasr$year_month == '1990-02', 'cpi_2025'])
df_gasr[df_gasr$year_month == '2025-07', 'cpi_2025'] = unique(df_gasr[df_gasr$year_month == '2025-06', 'cpi_2025'])

df_gasr$price_2025 = df_gasr$price * df_gasr$cpi_2025
```

## Time plots

```{r time-plots}
# plot gas prices in toronto over time
df_toronto = df_gasr |>
  filter(city == 'Toronto East/Est')

write_csv(x=df_toronto, file='data/processed/data_toronto_proc.csv')
# write toronto data 
plot_toronto = autoplot(object=df_toronto, price_2025) + 
  labs(y='Price (2025 dollars)') + 
  theme_classic()

ggsave("notebooks/figures/toronto_plot.png", plot = plot_toronto, width = 8, height = 6)
```


 
## Time series patterns 

1. **Trend**: a long-term change that does not have to be linear. 
2. **Seasonal**: changes that occur for fixed and known periods (e.g., holidays, seasons)
3. **Cyclic**: rises and falls that are not of a fixed frequency (e.g., economic conditions) and last at least 2 years. 

Plots below examine quarterly data. 

```{r r-quarterly}
aus_production |> 
  filter(year(Quarter) >= 1980) |>
  autoplot(Electricity) +
  labs(y='gWh', title='Australian electricity production') + 
  theme_classic()
```
![toronto plot](figures/toronto_plot.png){width=400px height=300px}

```{python python-quarterly}
(ggplot(r.aus_production, aes(x='Quarter', y='Electricity')) +
    geom_line() +
    labs(y='price') +
    theme_classic()
)
```


# Time series decomposition


Adjustments are often useful as they create simpler time series. There are four common types of adjustments: 

1) Calendar: Instead of computing average sales per month, reduce it down to sales/day, as the number of selling days differ by month. 
2) Population: Similarly, compute population variables at to either per capita or relative to some fixed denominator (1000)
3) Inflation: adjust financial data such that they are expressed in common dollars (i.e., adjusted for infation). 
4) Mathematical: if data are heteroscedastic, log transformations are useful (and also still interpretable). 

## Time series components

There are two types of decompositions: 

1. **Additive**. Value at any given $y_t$ time point is the sum of a seasonal ($S_t$), trend-cycle ($T_t$), and remainder ($R_t$) component. 

$$
y_t = S_t + T_t + R_t
$$

2. **Mulitplicative**. Value at any given $y_t$ time point is the product of a seasonal ($S_t$), trend-cycle ($T_t$), and remainder ($R_t$) component. 

$$
y_t = S_t \times T_t \times  R_t
$$

Additive decompoisiton is appropriate if neither $S_t$ or $T_t$ vary with the level of $y_t$. If not, multiplicative decomposition is more appropriate. Alternative, if there is heteroscedasity in either the seasonal or trend-cycle components, then transformations can be applied to instill stability in the data. On this point, note that using a log transformation is equivalent to using the multiplicative decomposition. 

### Examle: Employment in the US retail sector

```{r retail-plot}
us_retail_employment = us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)

autoplot(us_retail_employment, Employed) +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```


Code applies STL decomposition. NOtice that the `season_adjust` column represents the observed score without the seasonality component (i.e., `y_t - S_t`), but without the remainder. To better interpret changes, look only at the trend pattern. 

```{r stl-decomp}
dcmp <- us_retail_employment |>
  model(stl = STL(Employed))

stl_output = components(dcmp)

components(dcmp) |>
  as_tsibble() |>
  autoplot(Employed, colour="gray") +
  geom_line(aes(y=trend), colour = "#D55E00") +
  labs(
    y = "Persons (thousands)",
    title = "Total employment in US retail"
  )


# plots all three components 
# components(dcmp) |> autoplot()
```


## Moving averages

Moving average smoothing computes a moving average, $\hat{T}_t$ by taking the *k* observations that precede and follow the current *t* observation (note that $m = 2k+1$)

$$ 
\hat{T}_t = \frac{1}{m} \sum^k_{j=-k} y_{t+j}
$$

In R, the `slide_dbl()` function can be used to compute moving averages. 

```{r moving-average}
aus_exports <- global_economy |>
  filter(Country == "Australia") |>
  mutate(
    `5-MA` = slider::slide_dbl(Exports, mean,
                .before = 2, .after = 2, .complete = TRUE)
  )

aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports")
```

To obtain a centered series of moving averages, a moving can be taken of a moving average. This is commonly done with seasonal data. A more refined method is to use weighted moving averages where 

$$
\hat{T}_t = \sum^k_{j=-k} a_j y_{t+j}
$$

Weighted moving averages result in smoother trend cycles.

## Classical decomposition 

This is a simple decomposition and acts as a starting point for understanding time series analyses. As with STL decomposition, additive and multiplicative versions exist. The difference here is that the seasonal component, $S_t$, is assumed to be constant over time. 

### Additive decomposition

1. Use $2 /times m-MA$ is time series has even number of observations. 
2. Compute detrended time series, $y_t - \hat{T}_t$
3. Estimate the seasonal component, $\hat{S}_t$, by computing the mean value across all seasonal components across all years. For example, take the average of all $y_t - \hat{T}_t$ in February across all years. Importantly, the seasonal component values are then adjusted such that they sum to zero. Thinking here is that if the seasonal components do not sum to zero, then the excess will carry into the trend/noise. This is solved by mean centering. 
4. Compute remainder, $\hat{R}_t = y_t - \hat{T}_t - \hat{S}_t$. 

```{r classical-decom}
us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")
```

### Multiplicative decomposition

1. Use $2 /times m-MA$ is time series has even number of observations. 
2. Compute detrended time series, $\frac{y_t}{\hat{T}_t}$
3. Estimate the seasonal component, $\hat{S}_t$, by computing the mean value across all seasonal components across all years. For example, take the average of all $y_t - \hat{T}_t$ in February across all years. Importantly, the seasonal component values are then adjusted such that they sum to zero. Thinking here is that if the seasonal components do not sum to zero, then the excess will carry into the trend/noise. This is solved by mean centering. 
4. Compute remainder, $\hat{R}_t = \frac{y_t}{\hat{T}_t \hat{S}_t}$. 

Classical decomposition has several limitations: 

1. No estimate of first and last $m$ time periods for $\hat{T}$ or $\hat{R}$. 
2. Trend cycle, $\hat{T}$, gives overly smooth estimates that miss out on sudden changes. 
3. Assumption of constant $\hat{S}_t$ is tenuous. 
4. Cannot handle unusual incidences (e.g., worker strike causing)


## Methods used by official statistics agencies

Over time, several agencies have developed methods to deal with seasonal adjustments. These methods are largely variants of the X-11 and SEATS methods. Importantly, these methods are designed to deal with quarterly and monthly data and so cannot handle any other types of data (daily, weekly, hourly, etc.). 

### X-11 method

Improved classical decomposition to handle 

1. Missingness of trend values at first and last m values. 
2. Allows seasonal component to change over time. 
3. Can incorporate trading day variation, holidays, and effects of known preditros. 


```{r x11-vs-class}
plot_class = us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")

ggsave("notebooks/figures/class_add_employment.png", plot = plot_class, width = 8, height = 6)


x11_dcmp <- us_retail_employment |>
  model(x11 = X_13ARIMA_SEATS(Employed ~ x11(mode='add'))) |>
  components()
tsibb
plot_x11 = autoplot(x11_dcmp) +
  labs(title =
    "Decomposition of total US retail employment using X-11.")

ggsave("notebooks/figures/x11_add_employment.png", plot = plot_x11, width = 8, height = 6)
```

![Classical decomposition](figures/x11_add_employment.png){width=400px height=300px}
![X-11 method](figures/class_add_employment.png){width=400px height=300px}


THe X11 method has estimated trend values values at the beginning and end of the time period, has better overall resolution in the trend cycle, and captures the oddity in 1996 (within the remainder term). 

The seasonal values can also be examined across months across year year. 

```{r seasonal}
plot_seasonal = x11_dcmp |>
  gg_subseries(seasonal)

ggsave("notebooks/figures/x11_add_seasonal_plot.png", plot = plot_seasonal, width = 8, height = 6)
```

![Seasonal plot](figures/x11_add_seasonal_plot.png){width=400px height=300px}


### SEATS method

The Seasonal Extraction in ARIMA Time Series (SEATS) method  has the same three components as the X-11 method, but fits an ARIMA model to the data.  

```{r seats}

seats_dcmp <- us_retail_employment |>
  model(seats = X_13ARIMA_SEATS(Employed ~ seats())) |>
  components()
autoplot(seats_dcmp) +
  labs(title =
    "Decomposition of total US retail employment using SEATS")
```

## STL decomposition

Several advantages over SEATS and X-11 methods: 

1. Can handle any type of frequnecy of data. 
2. Seasonal component can change over time. 
3. Smoothness of trend-cylcle. 
4. Robust decomposition can be specified to instill robustness against outliers. Note this will affect the remainder component. 

Some disadvantages: 

1. Does not automatically handle trading day variation or calendar variation. 
2. Only has additive decomposition. That being said, logs of data can be computed to obtain a multiplicative decomoposition. Also, decompositions that lie between additive and multiplicative decompositions can be obtained by using Box-Cox transformation with $0 < \lambda < 1$. 

Two main parameters (that should both be odd numbers): 
1. Trend-cycle window: smaller windows capture sudden changes. 
2. Seasonal window: number of consecutive years used to estimate seasonal component. 

```{r stl}
us_retail_employment |>
  model(
    STL(Employed ~ trend(window = 7) +
                   season(window = "periodic"),
    robust = TRUE)) |>
  components() |>
  autoplot()
```


# Time series features

## STL Features

If data has a strong trend, then the variability of the seasonally-untrended data (removal of $S_t$ from $y_t$) should have more variability than the remainder ($R_t$). Conversely, if there is little trend-cycle in the data, $var(R)$ will largely relative to the total variability. This can be summarized in

$$ 
F_t = \max \big(0, \frac{var(R_t)}{var(R_t + T_t)})
$$

Note that an indicator function is used because $var{R_t}$ can be larger than $var{R_t + T_t}$. 

In a similar way, the strength of the seasonal component can also be examined. 
$$ 
F_s = \max \big(0, \frac{var(R_t)}{var(S_t + T_t)})
$$


```{r stl-features}
plot_trend_season = df_toronto[df_toronto] |>
  features(price_2025, feat_stl) |>
  ggplot(aes(x = trend_strength, y = seasonal_strength_year,
             col = `Fuel Type`)) +
  geom_point(size = 7) + 
  theme_classic()


ggsave("notebooks/figures/plot_trend_season.png", plot = plot_trend_season, width = 8, height = 6)
```

![Season-trend correlations](figures/plot_trend_season.png){width=400px height=300px}

1. Diesel has strongest seasonal and trend component 
2. Regular unleaded gasoline has weakest components

```{r seson-trend-plots}
df_5year <- df_toronto[df_toronto$Date >= as.Date("1995-01-01") & df_toronto$Date <= as.Date("1999-12-31"), ]


df_5year |> 
  filter(`Fuel Type` == 'Regular Unleaded Gasoline') |> 
  ggplot(aes(x=Date, y=price_2025)) + 
  geom_line(size=1) + 

  theme_classic()
```

# The forecaster's toolbox
## Fitted values and residuals 
Fitted values are not true forecasts if they use data points after the predicted value. 

*Innovation* residuals are simply on a transformed scale. Oftentimes, transformations will be needed. 

### Residual diagnostics 

Good forecasting methods will 

1. Provide uncorrelated residuals. If not, this suggets there is meaningful information that should be used in computing forecasts. 
2. Provide zero-mean residuals. If not, then forecasts are biased becase it is systematically under/overpredicting values. 

If forecasting method does not satisfy property, then it can be improved. That being said, a method that does satisfy these methods can still be improved. The following conditions are not ncessary, but also needed for computing prediction intervals . Importantly, it is not always possible to improve a model to satisfy these properties.

3. Innovation residuals are homoscedastic. Ensures prediction certainty is constant over time (for instance, during periods of volatility)
4. Innovation residuals are normally distributed. Ensures predictions intervals accurately capture uncertainty. 

### Example using gas prices

A naïve model is used for didactic purposes. In this model, a prediction is simply equal to the previous value in the series, which implies that residuals are simply the differnce between the current and previous value: 

$$
e_t = y_t - \hat{y}_t = y_t - y_{t-1}
$$

```{r naive-method}
plot_residuals = df_toronto |> 
  filter(city == 'Toronto East/Est', `Fuel Type` == 'Regular Unleaded Gasoline') |>
  model(NAIVE(price_2025)) |>
  gg_tsresiduals()


ggsave("notebooks/figures/residualplot.png", plot = plot_residuals, width = 8, height = 6)
```

In the residual plot, the residuals are normally ditributed, but ther are periods when variability and autocorrelated errors appear 

![Residual plot ](figures/residualplot.png){width=400px height=300px}

### Portmanteau tests for autoccorelation

Looking at ACF plots can be misleading because autocorrelations can appear to be outliers due to sampling error. To control against this, use Ljung-Box test 

$$
Q^* = T(T+2) \sum^l_{k=1} (T- k)^{-1} r^2_k, 
$$

where $T$ is the number of observations, $r_k$ is the autocorrelation for lag *k*, and $l$ is the maximum lag under consideration. This metric will have a $\chi^2$ distirbution if the autocorrelations come from a white noise series. 

In both cases, our residuals have some systematicity to them, indicating a poor model. 
```{r residual-test}
naive_model = df_toronto |> 
  filter(city == 'Toronto East/Est', `Fuel Type` == 'Regular Unleaded Gasoline') |>
  model(NAIVE(price_2025)) |>
  augment()
 
 naive_model |>
  features(.innov, box_pierce, lag = 10)

naive_model |>
  features(.innov, ljung_box, lag = 10)

```

## Distributional forecasts and prediction intervals 
### Prediction intervals 

Use the standard devation for *h*-step forecast to construct interval of certainty: 

$$
\hat{y}_{T + h|T} \pm c\hat{\sigma}_h
$$

### One-step prediction intervals

Intervals can be created using standard deviation of $t$ residuals

$$
\hat{\sigma} = \sqrt{\frac{1}{T-K-M} \sigma^T_{i=1}\epsilon^2_t}, 
$$

where $K$ is the number of parameters, $M$ is the number of missing values in the residuals (e.g., $M$ =1 for a naïve forecast because the first observation cannot be forecasted), and $T$ is the number of observations. 

### Multi-step prediction intervals 

For multi-step predictions, we cannot use the standard deviation of the residuals. More sophisticated approaches must be used. 

For simple models, the residual standard deviation ($\sigma$ is simply multiplied by a constant): 

1. Mean: $\hat{sigma}_h = \hat{\sigma} \sqrt{1 + 1/T}$
2. Naïve: $\hat{\sigma}_h = \hat{\sigma}\sqrt{h}$
3. Seasonal naïve: $\hat{\sigma}_h = \hat{\sigma}\sqrt{k+h}$
4. Drift: $\hat{\sigma}_h = \hat{\sigma}\sqrt{h (1 + h/(T-1))}$

Each of the above methods yield symmetric prediction intervals. 

```{r simple-forecast}
df_toronto = read_csv(file='data/processed/data_toronto_proc.csv')

df_toronto <- df_toronto |>
  filter(`Fuel Type` == 'Regular Unleaded Gasoline') |>
   mutate(week= yearweek(Date)) |>
  as_tsibble(index = week, key = c('city', `Fuel Type`))

pred_interval_plot = df_toronto |>
  filter(`Fuel Type` == 'Regular Unleaded Gasoline') |>
  model(NAIVE(price_2025)) |>
  forecast(h = 10) |>
 ggplot2::autoplot(df_toronto |> filter(Date > as.Date("2024-01-01"))) + 
 theme_classic()

 ggsave("notebooks/figures/pred_interval_plot.png", plot = pred_interval_plot, width = 8, height = 6)

```

### Prediction interbals from bootstrapped residuals 

When it is tenuous to assume that residuals are normally distributed, one solution is to create prediction intervals using bootstrapping methods. Importantly, bootstrapping assumes that residuals are uncorrelated and homoscedastic. 

With a naïve method, previous residuals are sampled in order to simulate the next obsservation. That is, when predicting beyond the *T* sampled time periods for a potential observation($y^\ast_{T+1}$),  

$$
y^\ast_{T+1} = y_T + \epsilon^\ast_{T+1}
$$

we sample $\epsilon^\ast_{T+1}$ from the set of previous residuals. Similarly, for the next observation, we can follow a similar procedure

$$
y^\ast_{T+2} = y_T + \epsilon^\ast_{T+2}
$$

The code below generates forcasts for the next 4 weeks. 

```{r bootstrap-interval}
fit <- df_toronto |>
  model(NAIVE(price_2025))

sim <- fit |> generate(h = 4, times = 5, bootstrap = TRUE)

plot_potentials = df_toronto |> 
  filter(Date > as.Date("2024-01-01"))|>
  ggplot(aes(x = week)) +
  geom_line(aes(y = price_2025)) +
  geom_line(aes(y = .sim, colour = as.factor(.rep)),
    data = sim) +
  labs(title="Weekly gas pump prices") +
  theme_classic() 

 ggsave("notebooks/figures/plot_potentials.png", plot = plot_potentials, width = 8, height = 6)


fit_boot = fit |> forecast(h = 4, bootstrap = TRUE)

plot_bootstrap = autoplot(fit_boot, df_toronto |> 
  filter(Date > as.Date("2024-01-01"))) +
  labs(title="Google daily closing stock price", y="$US" )

 ggsave("notebooks/figures/plot_bootstrap.png", plot = plot_bootstrap, width = 8, height = 6)
```


### Forecasting with decomposition

A simple way to express a decomposition is 

$$
y_t = \hat{S}_t + \hat{A}_t, 
$$

where $\hat{A}_t = \hat{T}_t\hat{R}_t$. Generally speaking, the seasonal component is assumed to unchanging or changing very slowly. 

### Evaluating point forecast accuracy 
 
Although analyzing residuals provides insight into whether a model can be improved, it does not give a reliable estimate of a model's performance on new data. To estimate test error, data need to be divided into a training and test set. The test set should contain at least as many time points as the forecast window. 

**Forecast errors** represent residuals obtained from the test data. That is, the residuals obtained on $h$ time periods after the $T$ time point where the difference is computed between te observed value at time $T + h$ and the predicted value at time $T + h$ using data up until time $T$. 

$$
\epsilon_{T + h} = y_{T + h} \hat{y}_{T+h|T}
$$

Note that forecast errors differ from residuals in two ways. First, residuals are computed on the training data. Second, residuals are based only on one-step forecasts, whereas forecast errors can involve multi-step forecasts. 

Errors can be one of three types: 

1. **Scale-dependent errors**. Errors are in units of variable. Some examples include 

$$
MAE = \text{mean}(|\epsilon_t|) \\
RMSE = \sqrt{text{mean}(\epsilon_t^2)}.
$$

Note that minimizing the MAE leads to forecasts of the median and minimizing the RMSE leads to forecasts of the mean. In other words, the RMSE is more appropriate when you want outliers to have large effects. 

2. **Percentage errors**. Error at each $t$ time point is conveyed as percentage of observation at $t$ time point. That is, 

$$
MAPE = \text{mean}(|100 (\frac{\epsilon_t}{y_t}))
$$

Three disadvantages: 
1. MAPE will blowup when $y_t$ is very close to zero and be undefined when $y_t$ is zero.
2. Assumes meaningful zero. For example, temperature measures fo F and C have arbitrary zero points. 
3. Asymmetric: treats the same absolute deviation differently depending on the size of the current value For example, a 5-point deviatin from a value of 10 is 50%, where it is onl 10% if the current value is 50. One historical solution was sMAPE, but this should be used because it can still blow up and can still be negative.  

3. **Scaled errors**. Proposed by Hyndman & Koehler (2006) as an improvement on percentage errors by scaling errors using the training data MAE from a simple forecast method. As an example, for non-seasonal time series, one simple model to use as a reference is a naïve model (where the prediction is simply the last value) 

$$
q_j = \frac{\epsilon_j}{\frac{1}{T-1} \sum^T_{t=2} |y_t - y_{t-1}|}
$$

For a seasonal naïve model, we'd simply take the value from last year at the same time (i.e., $m$ time periods ago).
$$
q_j = \frac{\epsilon_j}{\frac{1}{T-1} \sum^T_{t=2} |y_t - y_{t-m}|}
$$

The MASE or RMSSE can then be calcuated by, respectively, taking the average or square root of the sum of squared errors.  

### Examples

```{r model-selection}
recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)

beer_train <- recent_production |>
  filter(year(Quarter) <= 2007)

beer_fit <- beer_train |>
  model(
    Mean = MEAN(Beer),
    `Naive` = NAIVE(Beer),
    `Seasonal naive` = SNAIVE(Beer),
    Drift = RW(Beer ~ drift())
  )

beer_fc <- beer_fit |>
  forecast(h = 10)

beer_plot = beer_fc |>
  autoplot(
    aus_production |> filter(year(Quarter) >= 1992),
    level = NULL
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))

# NOTE: notice values of 1 for MASE and RMSSE on training data
accuracy(beer_fit)

accuracy(beer_fc, recent_production)
```

### Evaluating distributional forecast accuracy



