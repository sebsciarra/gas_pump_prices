---
title: "Forecasting: Principles and practice"
subtitle: "Chapters 2-3: Time series graphics and decomposition"
format: 
    pdf:
        number-sections: true 
        toc: true
execute: 
  echo: true
  output: false
engine: knitr
---


```{r data-sources}
# eval: false 
"""
CPI adjustment for inflation 
# https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1810000501&pickMembers%5B0%5D=1.2&cubeTimeFrame.startYear=1990&cubeTimeFrame.endYear=2024&referencePeriods=19900101%2C20240101
"""
```

```{r initiate-reticulate}
library('easypackages')
packages = c('reticulate', 'fpp3', 'tidyverse', 'fable', 'slider', 'seasonal')
libraries(packages)
# install.packages('feasts')
use_condaenv("base", required = TRUE)

setwd('/Users/seb/Desktop/Projects/gas_pump_prices/')
```

```{python loading-packages}
import os
import tarfile
import urllib
import pandas as pd
import numpy as np
from plotnine import * 

os.chdir('/Users/seb/Desktop/Projects/gas_pump_prices/')
```

```{python load-data-pyt}
df_gas = pd.read_csv('data/raw/fueltypesall.csv')

# convert date to datetime object 
df_gas['Date'] = pd.to_datetime(df_gas['Date'])
```


```{r load-data-r}
df_gasr = read_csv('data/raw/fueltypesall.csv')

# convert date to tsibble and then to long format
df_gasr <- df_gasr |>
  pivot_longer(cols = -c(Date, `Fuel Type`, `Type de carburant`), 
               names_to = "city", values_to = "price") |> 
  mutate(week= yearweek(Date)) |>
  as_tsibble(index = week, key = c('city', `Fuel Type`))
```

Apply inflation correction to gas prices. 

```{r inflation-corr}
# read in inflation data
df_inflation = read_csv('data/raw/1810000401_databaseLoadingData.csv')
df_gas_inf = df_inflation |> filter(`Products and product groups` == 'Gasoline')

df_gasr$year_month = format(df_gasr$Date, "%Y-%m")

# compute dollars in terms of 2025 dollars
# 1) Compute CPIs centered on 2025 
value_25 = df_gas_inf |> filter(REF_DATE == '2025-06') |> pull(VALUE)
df_gas_inf$cpi_2025 = value_25/df_gas_inf$VALUE

# merge in CPI value based on year and compute 
cols = c('REF_DATE', 'VALUE', 'cpi_2025')
df_gasr <- left_join(df_gasr, df_gas_inf[ ,cols], by = c("year_month" = "REF_DATE"))

# examine if tiem period with missing cpi_2025 value exists in inflatino prices
missing_time_periods = unique(df_test[is.na(df_test$cpi_2025), 'year_month'])

# NOTE: replace 1990-01 with 1990-02 and 2025-07 with 2025-06
df_gasr[df_gasr$year_month == '1990-01', 'cpi_2025'] = unique(df_gasr[df_gasr$year_month == '1990-02', 'cpi_2025'])
df_gasr[df_gasr$year_month == '2025-07', 'cpi_2025'] = unique(df_gasr[df_gasr$year_month == '2025-06', 'cpi_2025'])

df_gasr$price_2025 = df_gasr$price * df_gasr$cpi_2025
```

## Time plots

```{r time-plots}
# plot gas prices in toronto over time
df_toronto = df_gasr |>
  filter(city == 'Toronto East/Est')

plot_toronto = autoplot(object=df_toronto, price_2025) + 
  labs(y='Price (2025 dollars)') + 
  theme_classic()

ggsave("notebooks/figures/toronto_plot.png", plot = plot_toronto, width = 8, height = 6)
```


 
## Time series patterns 

1. **Trend**: a long-term change that does not have to be linear. 
2. **Seasonal**: changes that occur for fixed and known periods (e.g., holidays, seasons)
3. **Cyclic**: rises and falls that are not of a fixed frequency (e.g., economic conditions) and last at least 2 years. 

Plots below examine quarterly data. 

```{r r-quarterly}
aus_production |> 
  filter(year(Quarter) >= 1980) |>
  autoplot(Electricity) +
  labs(y='gWh', title='Australian electricity production') + 
  theme_classic()
```
![toronto plot](figures/toronto_plot.png){width=400px height=300px}

```{python python-quarterly}
(ggplot(r.aus_production, aes(x='Quarter', y='Electricity')) +
    geom_line() +
    labs(y='price') +
    theme_classic()
)
```


# Time series decomposition


Adjustments are often useful as they create simpler time series. There are four common types of adjustments: 

1) Calendar: Instead of computing average sales per month, reduce it down to sales/day, as the number of selling days differ by month. 
2) Population: Similarly, compute population variables at to either per capita or relative to some fixed denominator (1000)
3) Inflation: adjust financial data such that they are expressed in common dollars (i.e., adjusted for infation). 
4) Mathematical: if data are heteroscedastic, log transformations are useful (and also still interpretable). 

## Time series components

There are two types of decompositions: 

1. **Additive**. Value at any given $y_t$ time point is the sum of a seasonal ($S_t$), trend-cycle ($T_t$), and remainder ($R_t$) component. 

$$
y_t = S_t + T_t + R_t
$$

2. **Mulitplicative**. Value at any given $y_t$ time point is the product of a seasonal ($S_t$), trend-cycle ($T_t$), and remainder ($R_t$) component. 

$$
y_t = S_t \times T_t \times  R_t
$$

Additive decompoisiton is appropriate if neither $S_t$ or $T_t$ vary with the level of $y_t$. If not, multiplicative decomposition is more appropriate. Alternative, if there is heteroscedasity in either the seasonal or trend-cycle components, then transformations can be applied to instill stability in the data. On this point, note that using a log transformation is equivalent to using the multiplicative decomposition. 

### Examle: Employment in the US retail sector

```{r retail-plot}
us_retail_employment = us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)

autoplot(us_retail_employment, Employed) +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```


Code applies STL decomposition. NOtice that the `season_adjust` column represents the observed score without the seasonality component (i.e., `y_t - S_t`), but without the remainder. To better interpret changes, look only at the trend pattern. 

```{r stl-decomp}
dcmp <- us_retail_employment |>
  model(stl = STL(Employed))

stl_output = components(dcmp)

components(dcmp) |>
  as_tsibble() |>
  autoplot(Employed, colour="gray") +
  geom_line(aes(y=trend), colour = "#D55E00") +
  labs(
    y = "Persons (thousands)",
    title = "Total employment in US retail"
  )


# plots all three components 
# components(dcmp) |> autoplot()
```


## Moving averages

Moving average smoothing computes a moving average, $\hat{T}_t$ by taking the *k* observations that precede and follow the current *t* observation (note that $m = 2k+1$)

$$ 
\hat{T}_t = \frac{1}{m} \sum^k_{j=-k} y_{t+j}
$$

In R, the `slide_dbl()` function can be used to compute moving averages. 

```{r moving-average}
aus_exports <- global_economy |>
  filter(Country == "Australia") |>
  mutate(
    `5-MA` = slider::slide_dbl(Exports, mean,
                .before = 2, .after = 2, .complete = TRUE)
  )

aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports")
```

To obtain a centered series of moving averages, a moving can be taken of a moving average. This is commonly done with seasonal data. A more refined method is to use weighted moving averages where 

$$
\hat{T}_t = \sum^k_{j=-k} a_j y_{t+j}
$$

Weighted moving averages result in smoother trend cycles.

## Classical decomposition 

This is a simple decomposition and acts as a starting point for understanding time series analyses. As with STL decomposition, additive and multiplicative versions exist. The difference here is that the seasonal component, $S_t$, is assumed to be constant over time. 

### Additive decomposition

1. Use $2 /times m-MA$ is time series has even number of observations. 
2. Compute detrended time series, $y_t - \hat{T}_t$
3. Estimate the seasonal component, $\hat{S}_t$, by computing the mean value across all seasonal components across all years. For example, take the average of all $y_t - \hat{T}_t$ in February across all years. Importantly, the seasonal component values are then adjusted such that they sum to zero. Thinking here is that if the seasonal components do not sum to zero, then the excess will carry into the trend/noise. This is solved by mean centering. 
4. Compute remainder, $\hat{R}_t = y_t - \hat{T}_t - \hat{S}_t$. 

```{r classical-decom}
us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")
```

### Multiplicative decomposition

1. Use $2 /times m-MA$ is time series has even number of observations. 
2. Compute detrended time series, $\frac{y_t}{\hat{T}_t}$
3. Estimate the seasonal component, $\hat{S}_t$, by computing the mean value across all seasonal components across all years. For example, take the average of all $y_t - \hat{T}_t$ in February across all years. Importantly, the seasonal component values are then adjusted such that they sum to zero. Thinking here is that if the seasonal components do not sum to zero, then the excess will carry into the trend/noise. This is solved by mean centering. 
4. Compute remainder, $\hat{R}_t = \frac{y_t}{\hat{T}_t \hat{S}_t}$. 

Classical decomposition has several limitations: 

1. No estimate of first and last $m$ time periods for $\hat{T}$ or $\hat{R}$. 
2. Trend cycle, $\hat{T}$, gives overly smooth estimates that miss out on sudden changes. 
3. Assumption of constant $\hat{S}_t$ is tenuous. 
4. Cannot handle unusual incidences (e.g., worker strike causing)


## Methods used by official statistics agencies

Over time, several agencies have developed methods to deal with seasonal adjustments. These methods are largely variants of the X-11 and SEATS methods. Importantly, these methods are designed to deal with quarterly and monthly data and so cannot handle any other types of data (daily, weekly, hourly, etc.). 

### X-11 method

Improved classical decomposition to handle 

1. Missingness of trend values at first and last m values. 
2. Allows seasonal component to change over time. 
3. Can incorporate trading day variation, holidays, and effects of known preditros. 


```{r x11-vs-class}
plot_class = us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")

ggsave("notebooks/figures/class_add_employment.png", plot = plot_class, width = 8, height = 6)


x11_dcmp <- us_retail_employment |>
  model(x11 = X_13ARIMA_SEATS(Employed ~ x11(mode='add'))) |>
  components()

plot_x11 = autoplot(x11_dcmp) +
  labs(title =
    "Decomposition of total US retail employment using X-11.")

ggsave("notebooks/figures/x11_add_employment.png", plot = plot_x11, width = 8, height = 6)
```

![Classical decomposition](figures/x11_add_employment.png){width=400px height=300px}
![X-11 method](figures/class_add_employment.png){width=400px height=300px}


THe X11 method has estimated trend values values at the beginning and end of the time period, has better overall resolution in the trend cycle, and captures the oddity in 1996 (within the remainder term). 

The seasonal values can also be examined across months across year year. 

```{r seasonal}
plot_seasonal = x11_dcmp |>
  gg_subseries(seasonal)

ggsave("notebooks/figures/x11_add_seasonal_plot.png", plot = plot_seasonal, width = 8, height = 6)
```

![Seasonal plot](figures/x11_add_seasonal_plot.png){width=400px height=300px}


### SEATS method

The Seasonal Extraction in ARIMA Time Series (SEATS) method  has the same three components as the X-11 method, but fits an ARIMA model to the data.  

```{r seats}

seats_dcmp <- us_retail_employment |>
  model(seats = X_13ARIMA_SEATS(Employed ~ seats())) |>
  components()
autoplot(seats_dcmp) +
  labs(title =
    "Decomposition of total US retail employment using SEATS")
```

## STL decomposition

Several advantages over SEATS and X-11 methods: 

1. Can handle any type of frequnecy of data. 
2. Seasonal component can change over time. 
3. Smoothness of trend-cylcle. 
4. Robust decomposition can be specified to instill robustness against outliers. Note this will affect the remainder component. 

Some disadvantages: 

1. Does not automatically handle trading day variation or calendar variation. 
2. Only has additive decomposition. That being said, logs of data can be computed to obtain a multiplicative decomoposition. Also, decompositions that lie between additive and multiplicative decompositions can be obtained by using Box-Cox transformation with $0 < \lambda < 1$. 

Two main parameters (that should both be odd numbers): 
1. Trend-cycle window: smaller windows capture sudden changes. 
2. Seasonal window: number of consecutive years used to estimate seasonal component. 

```{r stl}
us_retail_employment |>
  model(
    STL(Employed ~ trend(window = 7) +
                   season(window = "periodic"),
    robust = TRUE)) |>
  components() |>
  autoplot()
```


# Time series features

## STL Features

If data has a strong trend, then the variability of the seasonally-untrended data (removal of $S_t$ from $y_t$) should have more variability than the remainder ($R_t$). Conversely, if there is little trend-cycle in the data, $var(R)$ will largely relative to the total variability. This can be summarized in

$$ 
F_t = \max \big(0, \frac{var(R_t)}{var(R_t + T_t)})
$$

Note that an indicator function is used because $var{R_t}$ can be larger than $var{R_t + T_t}$. 

In a similar way, the strength of the seasonal component can also be examined. 
$$ 
F_s = \max \big(0, \frac{var(R_t)}{var(S_t + T_t)})
$$


```{r stl-features}
plot_trend_season = df_toronto[df_toronto] |>
  features(price_2025, feat_stl) |>
  ggplot(aes(x = trend_strength, y = seasonal_strength_year,
             col = `Fuel Type`)) +
  geom_point(size = 7) + 
  theme_classic()


ggsave("notebooks/figures/plot_trend_season.png", plot = plot_trend_season, width = 8, height = 6)
```

![Season-trend correlations](figures/plot_trend_season.png){width=400px height=300px}

1. Diesel has strongest seasonal and trend component 
2. Regular unleaded gasoline has weakest components

```{r seson-trend-plots}
df_5year <- df_toronto[df_toronto$Date >= as.Date("1995-01-01") & df_toronto$Date <= as.Date("1999-12-31"), ]


df_5year |> 
  filter(`Fuel Type` == 'Regular Unleaded Gasoline') |> 
  ggplot(aes(x=Date, y=price_2025)) + 
  geom_line(size=1) + 

  theme_classic()
```

# The forecaster's toolbox
## Fitted values and residuals 
Fitted values are not true forecasts if they use data points after the predicted value. 

*Innovation* residuals are simply on a transformed scale. Oftentimes, transformations will be needed. 

### Residual diagnostics 

Good forecasting methods will 

1. Provide uncorrelated residuals. If not, this suggets there is meaningful information that should be used in computing forecasts. 
2. Provide zero-mean residuals. If not, then forecasts are biased becase it is systematically under/overpredicting values. 

If forecasting method does not satisfy property, then it can be improved. That being said, a method that does satisfy these methods can still be improved. The following conditions are not ncessary, but also needed for computing prediction intervals . Importantly, it is not always possible to improve a model to satisfy these properties.

3. Innovation residuals are homoscedastic. Ensures prediction certainty is constant over time (for instance, during periods of volatility)
4. Innovation residuals are normally distributed. Ensures predictions intervals accurately capture uncertainty. 

### Example using gas prices

A naïve model is used for didactic purposes. In this model, a prediction is simply equal to the previous value in the series, which implies that residuals are simply the differnce between the current and previous value: 

$$
e_t = y_t - \hat{y}_t = y_t - y_{t-1}
$$

```{r naive-method}
plot_residuals = df_toronto |> 
  filter(city == 'Toronto East/Est', `Fuel Type` == 'Regular Unleaded Gasoline') |>
  model(NAIVE(price_2025)) |>
  gg_tsresiduals()


ggsave("notebooks/figures/residualplot.png", plot = plot_residuals, width = 8, height = 6)
```

In the residual plot, the residuals are normally ditributed, but ther are periods when variability and autocorrelated errors appear 

![Residual plot ](figures/residualplot.png){width=400px height=300px}

### Portmanteau tests for autoccorelation

Looking at ACF plots can be misleading because autocorrelatinos can appear to be outliers due to sampling error. To control against this, use Ljung-Box test 

$$
Q^* = T(T+2) \sum^l_{k=1} (T- k)^{-1} r^2_k, 
$$

where $T$ is the number of observatinos, $r_k$ is the autocorrelation for lag *k*, and $l$ is the maximum lag under consideration. This metric will have a $\chi^2$ distirbution if the autocorrelations come from a white noise series. 

In both cases, our residuals have some systematicity to them, indicating a poor model. 
```{r residual-test}
naive_model = df_toronto |> 
  filter(city == 'Toronto East/Est', `Fuel Type` == 'Regular Unleaded Gasoline') |>
  model(NAIVE(price_2025)) |>
  augment()
 
 naive_model |>
  features(.innov, box_pierce, lag = 10)

naive_model |>
  features(.innov, ljung_box, lag = 10)

```

## Distributional forecasts and prediction intervals 

